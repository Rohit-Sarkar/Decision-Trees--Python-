{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the make_moons datasets and applying the Random Forest, Logistic Regresssion, SVM and ensemble of all three as Voting Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X,y = datasets.make_moons(n_samples=1500,noise=0.3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('rf', RandomF...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "voting_clf = VotingClassifier(\n",
    "                estimators=[('lr',log_clf),('rf',rnd_clf),('svc',svm_clf)],\n",
    "                voting = 'hard'\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.862222222222\n",
      "RandomForestClassifier 0.893333333333\n",
      "SVC 0.915555555556\n",
      "VotingClassifier 0.911111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf,rnd_clf,svm_clf,voting_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Voting Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The difference between the hard and soft voting classifier is that in the hard voting classifier, the majority of the predicted class from the different classifier is taken as predicted class whereas in the soft voting classifier , the averaged over all the individual classifiers is taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X,y = datasets.make_moons(n_samples=1500,noise=0.3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('rf', RandomF...',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability=True)\n",
    "voting_clf = VotingClassifier(\n",
    "                estimators=[('lr',log_clf),('rf',rnd_clf),('svc',svm_clf)],\n",
    "                voting = 'soft'\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.862222222222\n",
      "RandomForestClassifier 0.893333333333\n",
      "SVC 0.915555555556\n",
      "VotingClassifier 0.908888888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf,rnd_clf,svm_clf,voting_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Pasting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging is done with replacement where Pasting is done without replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# n_jobs means using all the available cores of CPU, n_estimators are the number of decision trees to be modelled, max_samples \n",
    "# are the number of samples to be taken for one model.\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "            DecisionTreeClassifier(), n_estimators=500,\n",
    "            max_samples = 100, bootstrap=True,n_jobs=-1\n",
    ")\n",
    "\n",
    "bag_clf.fit(X_train,y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bagging  0.913333333333\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Bagging ', accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Pasting  0.911111111111\n"
     ]
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "            DecisionTreeClassifier(), n_estimators=500,\n",
    "            max_samples = 100, bootstrap=False,n_jobs=-1\n",
    ")\n",
    "\n",
    "bag_clf.fit(X_train,y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print('Accuracy of Pasting ', accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the prediction of out of bag instances and theoritically the oob score and accuracy are very similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90571428571428569"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "            DecisionTreeClassifier(), n_estimators=500,\n",
    "            max_samples = 100, bootstrap=True,n_jobs=-1, oob_score=True)\n",
    "bag_clf.fit(X_train,y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.88789238,  0.11210762],\n",
       "       [ 0.95384615,  0.04615385],\n",
       "       [ 0.96982759,  0.03017241],\n",
       "       ..., \n",
       "       [ 0.93791574,  0.06208426],\n",
       "       [ 0.68777293,  0.31222707],\n",
       "       [ 0.00883002,  0.99116998]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score  0.911111111111\n"
     ]
    }
   ],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "print('Accuracy Score ',accuracy_score(y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extremely Randomized Trees (Extra Trees) is much faster than Random Forest as it uses random thresholds for each feature rather than finding the best possible threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score  0.906666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "ext_clf = ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=16,n_jobs=-1)\n",
    "ext_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ext = ext_clf.predict(X_test)\n",
    "print('Accuracy Score ',accuracy_score(y_test,y_pred_ext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to find the important features in Random Forest , similarly we can find in Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.108359487526\n",
      "sepal width (cm) 0.0256364264132\n",
      "petal length (cm) 0.461047860704\n",
      "petal width (cm) 0.404956225356\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"],iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### AdaBoosting also known as adaptive boosting in which first base classifier is trained and used to make prediction on all of the training set , there are few instances which are not predicted well , so relative weights to those points are increased and pobability of choosing those points increases for the next classifier and again the prediction is made and these steps are repeated until we get smallest prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of AdaBoost(SAMME method) 0.902222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Multiclass version of adaboost is SAMME (Stagewise Adaptive Modelling using a Multiclass Exponential Loss Function), in the\n",
    "# class of binary class it performs same as adaboost.\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "                DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "                algorithm = \"SAMME\" , learning_rate=0.5\n",
    ")\n",
    "\n",
    "ada_clf.fit(X_train,y_train)\n",
    "y_pred_ada = ada_clf.predict(X_test)\n",
    "print(\"Accuracy Score of AdaBoost(SAMME method)\", accuracy_score(y_test,y_pred_ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of AdaBoost(SAMME.R method) 0.884444444444\n"
     ]
    }
   ],
   "source": [
    "# SAMME.R (Stagewise Adaptive Modelling using a Multiclass Exponential Loss Function. Real), finds the class probabilities \n",
    "# in case of classifier which can predict class probabilities.\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "                DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "                algorithm = \"SAMME.R\" , learning_rate=0.5\n",
    ")\n",
    "\n",
    "ada_clf.fit(X_train,y_train)\n",
    "y_pred_ada = ada_clf.predict(X_test)\n",
    "print(\"Accuracy Score of AdaBoost(SAMME.R method)\", accuracy_score(y_test,y_pred_ada))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradient boosting works similar to AdaBoost, but it tries to fit the new predictor to the residual errors obtained from the previous predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X,y)\n",
    "\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X,y2)\n",
    "\n",
    "y3 = y - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X,y3)\n",
    "\n",
    "y_pred = sum(tree.predict(X) for tree in (tree_reg1,tree_reg2,tree_reg3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06975585,  0.06975585,  0.63768116, ...,  0.62518239,\n",
       "        0.06975585,  0.05040951])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00671784,  0.07861027,  0.93049823, ...,  0.9665889 ,\n",
       "        0.00671784,  0.14284551])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators = 3, learning_rate = 1.0)\n",
    "gbrt.fit(X,y)\n",
    "gbrt.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping in gradient boosting using staged_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of normal Gradient Boosting  0.0703353697942\n",
      "MSE of best Gradient Boosting  0.0697126831512\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "errors = [mean_squared_error(y_val,y_pred)\n",
    "         for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train,y_train)\n",
    "\n",
    "y_pred1 = gbrt.predict(X_val)\n",
    "y_pred2 = gbrt_best.predict(X_val)\n",
    "\n",
    "print(\"MSE of normal Gradient Boosting \", mean_squared_error(y_val,y_pred1))\n",
    "print(\"MSE of best Gradient Boosting \", mean_squared_error(y_val,y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the above code, first all the trees are modeled then we find optimal number of trees to find the best solution. But it takes lot of time in case of large dataset , therefore we can use another approach where we can stop training the model when the validation error stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE :  0.0704562158102\n"
     ]
    }
   ],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2,warm_start = True)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1,120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train,y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val,y_pred)\n",
    "    # print(val_error)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break # early stopping\n",
    "print('MSE : ',val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting actually randomly samples the training set and uses small amount of the instances to train the model.\n",
    "\n",
    "### As in the below code , we have used subsample as 0.25 which means that it take 25% of the training set for training the model in each iteration. It is faster than the normal gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE :  0.0755182860675\n"
     ]
    }
   ],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2,warm_start = True,subsample=0.25)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1,120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train,y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val,y_pred)\n",
    "    # print(val_error)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break # early stopping\n",
    "print('MSE : ',val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
